<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: lvs | Code Mind]]></title>
  <link href="http://gekben.gitcd.com/blog/categories/lvs/atom.xml" rel="self"/>
  <link href="http://gekben.gitcd.com/"/>
  <updated>2013-03-19T08:31:40+08:00</updated>
  <id>http://gekben.gitcd.com/</id>
  <author>
    <name><![CDATA[Kevin Zeng]]></name>
    <email><![CDATA[kevintech08@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[基于lvs的全透明传输思考]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/11/13/full-transparent-with-lvs/"/>
    <updated>2012-11-13T17:28:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/11/13/full-transparent-with-lvs</id>
    <content type="html"><![CDATA[<p>互联网公司一般使用LB的方式，是把Web App Server作为real server，最简单的情况是请求在这里终结，响应由Web App Server返回，一般web server不用再访问外网了。再复杂的情况，就
是Web App Server又作为client去请求内网的其它资源，如DB，其它Web Server等，在这种典型的互联网部署模式下，一般可以把Web Server看作终点。</p>

<p>还有一种部署方式，load balancing的对象不是Web App Server，而是firewall系统，比如用linux搭建的firewall，经过firewall的流量需要全透明传输，即不修改layer 2以上的信息，firewall
对过滤后合格的包做forwarding。在这种部署模式下，LB也需要有全透明传输的能力，画个图解释一下:</p>

<p>{% imgpopup /images/post_img/lvs_trans_firewall.png 70%  %}</p>

<!-- more -->


<p>这里所说的LB都是lvs，以http为例，首先client请求WWW Server，client有自己的公网地址，组装好的ip header，source ip是cip，destination ip是www_ip，
source port是cport，destination port 80，lvs要支持这种destination ip非vip的部署，需要使用到fwmark。
首先是要mark出这个请求：
{% codeblock mark the traffic lang:bash %}</p>

<h1>iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-xmark 1</h1>

<p>{% endcodeblock %}</p>

<p>然后交由lvs处理：
{% codeblock direct traffic to local_in hook lang:bash %}</p>

<h1>ip rule add prio 101 fwmark 1 table 101</h1>

<h1>ip route add local 0/0 dev lo table 10</h1>

<p>{% endcodeblock %}</p>

<p>lvs的fwmark service及real server：
{% codeblock define lvs fwmark service and real servers hook:bash %}</p>

<h1>ipvsadm -A -f 1 -p #default scheduler is wlc,enable persistent</h1>

<h1>ipvsadm -a -f 1 -r rs1 #default xmit method is direct routing</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<p>{% endcodeblock %}</p>

<p>这样配置后，LB1充当了透明的load balancer，经过LB1的request的layer2以上的信息都没有变化，rs1~rs4都可以接收请求，
并进行过滤处理，也是全透明的。先考虑没有LB2的情况，request从rs出来，被转发到Router，Router在将request转发至外网，
直至WWW_Server。从Client到WWW_server，layer2以上的地址和端口信息都没有改变。</p>

<p>Response从WWW_Server出发，source是www_ip：80，destination是cip:cport，发向Router，Router这时候不知道该
如何进行路由了，因为从rs1出来的request，对应的response也要回到rs1，Router没有rs1的内部ip，也没有连接信息
参考，response回不到对应real server，连接无法建立起来。</p>

<p>产生这个问题的原因是real server是有状态的，在request经过它时做了记录，例如linux的防火墙的基础conn_track，它期待response
也必须经过这台real server，tcp/udp以及基于它们的应用层协议都是如此。为了让response能回到正确的real server，需要利用到一种技术，
所有request和response都要经过它转发，在request进来时，记录它的源mac地址和接收包的device，response回来时，绕过路由直接发送给
之前记录的源mac，维护连接表+mac地址是必须的。</p>

<p>目前支持这种技术的设备有F5，被称为Auto Last Hop，还有Juniper的某些设备，称作Reverse Route。把这种设备部署在LB2的位置，即可实现
full transparent，当然，也不是一定要单独搞一台LB2，用一台LB既做Load Balancer又做反向路由也是可以的。使用这种设备的最大缺点就是，
贵。于是想基于Lvs实现方向路由功能，想想也是非常合理的，lvs本身可以很好的维护管理连接状态，它有一张connection table，连接状态迁移，
超时机制，垃圾回收都已经非常成熟，再加上source mac和in_dev并不难。需要注意的是，lvs在这种使用模式下，应该定义fwmark service，它的rs只有一个，
就是下一跳的ip。经过一段时间的开发，对lvs进行修改后，基于lvs的反向路由功能已经OK，准备下一篇来讲讲细节。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LVS peristent代码分析]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/07/11/study-on-lvs-kernel-code/"/>
    <updated>2012-07-11T12:24:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/07/11/study-on-lvs-kernel-code</id>
    <content type="html"><![CDATA[<p>一直纠结于LVS使用persistent时，是以何为依据，决定一个新的连接请求的命运的：是被persistent管理，去到之前的real server，还是被调度算法重新调度，去到新的real server。
代码里其实写得非常清楚：</p>

<h2>Persistent in LVS(ipvs)</h2>

<ul>
<li>fwmark

<ul>
<li>&lt;IPPROTO_IP,caddr,0,fwmark,0,daddr,0> 这个六元组，最开始是写死的
ip协议，第一个0是cport，第二个0是dport，就是不在乎cport和dport，
这个daddr值得一提，经过debug发现，这个值为<code>0.0.0.fwmark</code>。</li>
</ul>
</li>
<li>Port zero service &lt;protocol,caddr,0,vaddr,0,daddr,0></li>
<li>non Port zero service

<ul>
<li>FTP &lt;caddr,0,vaddr,0,daddr,0></li>
<li>NON-FTP &lt;caddr,0,vaddr,vport,daddr,dport></li>
</ul>
</li>
</ul>


<!-- more -->


<p>  <br/>
如果是transparent mode，这种透明模式一般都是通过fwmark的方式实现，客户
端是不知道vip的存在的，比如用<code>iptables</code>设置了<code>fwmark</code>为3，则访问
http://sina.com.cn时，<code>daddr</code>就是<code>0.0.0.3</code>。</p>

<p>见代码17，21，77，80行。
{% codeblock /net/netfilter/ipvs/ip_vs_core.c lang:c %}</p>

<pre><code>/*
 * As far as we know, FTP is a very complicated network protocol, and
 * it uses control connection and data connections. For active FTP,
 * FTP server initialize data connection to the client, its source port
 * is often 20. For passive FTP, FTP server tells the clients the port
 * that it passively listens to,  and the client issues the data
 * connection. In the tunneling or direct routing mode, the load
 * balancer is on the client-to-server half of connection, the port
 * number is unknown to the load balancer. So, a conn template like
 * &lt;caddr, 0, vaddr, 0, daddr, 0&gt; is created for persistent FTP
 * service, and a template like &lt;caddr, 0, vaddr, vport, daddr, dport&gt;
 * is created for other persistent services.
 */
if (ports[1] == svc-&gt;port) {
    /* Check if a template already exists */
    if (svc-&gt;port != FTPPORT)
        ct = ip_vs_ct_in_get(svc-&gt;af, iph.protocol, &amp;snet, 0,
                     &amp;iph.daddr, ports[1]); /* &lt;caddr,0,vaddr,vport,daddr,dport&gt; */
    else
        ct = ip_vs_ct_in_get(svc-&gt;af, iph.protocol, &amp;snet, 0,
                     &amp;iph.daddr, 0);        /* &lt;caddr,0,vaddr,0,daddr,0&gt; */

    if (!ct || !ip_vs_check_template(ct)) {
        /*
         * No template found or the dest of the connection
         * template is not available.
         */
        dest = svc-&gt;scheduler-&gt;schedule(svc, skb);
        if (dest == NULL) {
            IP_VS_DBG(1, "p-schedule: no dest found.\n");
            return NULL;
        }

        /*
         * Create a template like &lt;protocol,caddr,0,
         * vaddr,vport,daddr,dport&gt; for non-ftp service,
         * and &lt;protocol,caddr,0,vaddr,0,daddr,0&gt;
         * for ftp service.
         */
        if (svc-&gt;port != FTPPORT)
            ct = ip_vs_conn_new(svc-&gt;af, iph.protocol,
                        &amp;snet, 0,
                        &amp;iph.daddr,
                        ports[1],
                        &amp;dest-&gt;addr, dest-&gt;port,
                        IP_VS_CONN_F_TEMPLATE,
                        dest);
        else
            ct = ip_vs_conn_new(svc-&gt;af, iph.protocol,
                        &amp;snet, 0,
                        &amp;iph.daddr, 0,
                        &amp;dest-&gt;addr, 0,
                        IP_VS_CONN_F_TEMPLATE,
                        dest);
        if (ct == NULL)
            return NULL;

        ct-&gt;timeout = svc-&gt;timeout;
    } else {
        /* set destination with the found template */
        dest = ct-&gt;dest;
    }
    dport = dest-&gt;port;
} else {
    /*
     * Note: persistent fwmark-based services and persistent
     * port zero service are handled here.
     * fwmark template: &lt;IPPROTO_IP,caddr,0,fwmark,0,daddr,0&gt;
     * port zero template: &lt;protocol,caddr,0,vaddr,0,daddr,0&gt;
     */
    if (svc-&gt;fwmark) {
        union nf_inet_addr fwmark = {
            .ip = htonl(svc-&gt;fwmark)
        };

        ct = ip_vs_ct_in_get(svc-&gt;af, IPPROTO_IP, &amp;snet, 0,
                     &amp;fwmark, 0); /* &lt;IPPROTO_IP,caddr,0,fwmark,0,daddr,0&gt; */
    } else
        ct = ip_vs_ct_in_get(svc-&gt;af, iph.protocol, &amp;snet, 0,
                     &amp;iph.daddr, 0); /* &lt;protocol,caddr,0,vaddr,0,daddr,0&gt; */

    if (!ct || !ip_vs_check_template(ct)) {
        /*
         * If it is not persistent port zero, return NULL,
         * otherwise create a connection template.
         */
        if (svc-&gt;port)
            return NULL;

        dest = svc-&gt;scheduler-&gt;schedule(svc, skb);
        if (dest == NULL) {
            IP_VS_DBG(1, "p-schedule: no dest found.\n");
            return NULL;
        }

        /*
         * Create a template according to the service
         */
        if (svc-&gt;fwmark) {
            union nf_inet_addr fwmark = {
                .ip = htonl(svc-&gt;fwmark)
            };

            ct = ip_vs_conn_new(svc-&gt;af, IPPROTO_IP,
                        &amp;snet, 0,
                        &amp;fwmark, 0,
                        &amp;dest-&gt;addr, 0,
                        IP_VS_CONN_F_TEMPLATE,
                        dest);
        } else
            ct = ip_vs_conn_new(svc-&gt;af, iph.protocol,
                        &amp;snet, 0,
                        &amp;iph.daddr, 0,
                        &amp;dest-&gt;addr, 0,
                        IP_VS_CONN_F_TEMPLATE,
                        dest);
        if (ct == NULL)
            return NULL;

        ct-&gt;timeout = svc-&gt;timeout;
    } else {
        /* set destination with the found template */
        dest = ct-&gt;dest;
    }
    dport = ports[1];
}
</code></pre>

<p>{% endcodeblock %}</p>
]]></content>
  </entry>
  
</feed>
