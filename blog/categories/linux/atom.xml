<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Code Mind]]></title>
  <link href="http://gekben.gitcd.com/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://gekben.gitcd.com/"/>
  <updated>2013-03-19T08:31:40+08:00</updated>
  <id>http://gekben.gitcd.com/</id>
  <author>
    <name><![CDATA[Kevin Zeng]]></name>
    <email><![CDATA[kevintech08@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[基于lvs的全透明传输思考]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/11/13/full-transparent-with-lvs/"/>
    <updated>2012-11-13T17:28:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/11/13/full-transparent-with-lvs</id>
    <content type="html"><![CDATA[<p>互联网公司一般使用LB的方式，是把Web App Server作为real server，最简单的情况是请求在这里终结，响应由Web App Server返回，一般web server不用再访问外网了。再复杂的情况，就
是Web App Server又作为client去请求内网的其它资源，如DB，其它Web Server等，在这种典型的互联网部署模式下，一般可以把Web Server看作终点。</p>

<p>还有一种部署方式，load balancing的对象不是Web App Server，而是firewall系统，比如用linux搭建的firewall，经过firewall的流量需要全透明传输，即不修改layer 2以上的信息，firewall
对过滤后合格的包做forwarding。在这种部署模式下，LB也需要有全透明传输的能力，画个图解释一下:</p>

<p>{% imgpopup /images/post_img/lvs_trans_firewall.png 70%  %}</p>

<!-- more -->


<p>这里所说的LB都是lvs，以http为例，首先client请求WWW Server，client有自己的公网地址，组装好的ip header，source ip是cip，destination ip是www_ip，
source port是cport，destination port 80，lvs要支持这种destination ip非vip的部署，需要使用到fwmark。
首先是要mark出这个请求：
{% codeblock mark the traffic lang:bash %}</p>

<h1>iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-xmark 1</h1>

<p>{% endcodeblock %}</p>

<p>然后交由lvs处理：
{% codeblock direct traffic to local_in hook lang:bash %}</p>

<h1>ip rule add prio 101 fwmark 1 table 101</h1>

<h1>ip route add local 0/0 dev lo table 10</h1>

<p>{% endcodeblock %}</p>

<p>lvs的fwmark service及real server：
{% codeblock define lvs fwmark service and real servers hook:bash %}</p>

<h1>ipvsadm -A -f 1 -p #default scheduler is wlc,enable persistent</h1>

<h1>ipvsadm -a -f 1 -r rs1 #default xmit method is direct routing</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<p>{% endcodeblock %}</p>

<p>这样配置后，LB1充当了透明的load balancer，经过LB1的request的layer2以上的信息都没有变化，rs1~rs4都可以接收请求，
并进行过滤处理，也是全透明的。先考虑没有LB2的情况，request从rs出来，被转发到Router，Router在将request转发至外网，
直至WWW_Server。从Client到WWW_server，layer2以上的地址和端口信息都没有改变。</p>

<p>Response从WWW_Server出发，source是www_ip：80，destination是cip:cport，发向Router，Router这时候不知道该
如何进行路由了，因为从rs1出来的request，对应的response也要回到rs1，Router没有rs1的内部ip，也没有连接信息
参考，response回不到对应real server，连接无法建立起来。</p>

<p>产生这个问题的原因是real server是有状态的，在request经过它时做了记录，例如linux的防火墙的基础conn_track，它期待response
也必须经过这台real server，tcp/udp以及基于它们的应用层协议都是如此。为了让response能回到正确的real server，需要利用到一种技术，
所有request和response都要经过它转发，在request进来时，记录它的源mac地址和接收包的device，response回来时，绕过路由直接发送给
之前记录的源mac，维护连接表+mac地址是必须的。</p>

<p>目前支持这种技术的设备有F5，被称为Auto Last Hop，还有Juniper的某些设备，称作Reverse Route。把这种设备部署在LB2的位置，即可实现
full transparent，当然，也不是一定要单独搞一台LB2，用一台LB既做Load Balancer又做反向路由也是可以的。使用这种设备的最大缺点就是，
贵。于是想基于Lvs实现方向路由功能，想想也是非常合理的，lvs本身可以很好的维护管理连接状态，它有一张connection table，连接状态迁移，
超时机制，垃圾回收都已经非常成熟，再加上source mac和in_dev并不难。需要注意的是，lvs在这种使用模式下，应该定义fwmark service，它的rs只有一个，
就是下一跳的ip。经过一段时间的开发，对lvs进行修改后，基于lvs的反向路由功能已经OK，准备下一篇来讲讲细节。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建基于User Mode Linux网络模块开发环境]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/11/13/build-uml-development-environment/"/>
    <updated>2012-11-13T15:40:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/11/13/build-uml-development-environment</id>
    <content type="html"><![CDATA[<p>最近在写一些内核网络模块，需要一个好用的编译、调试、测试环境，选用了uml，记录一下设置多台uml和host通信，并share fs。
uml基本安装和使用可以参考<a href="https://wiki.archlinux.org/index.php/User-mode_Linux#Setup_by_rootfs_.2B_tap">archlinux uml</a>。
{% codeblock start uml with vde support lang:bash %}</p>

<h1>先启动虚拟switch</h1>

<h1>vde_switch -s /tmp/switch1 -tap tap0 -m 666</h1>

<h1>启动一台单网卡的uml，已经与之前定义的switch绑定</h1>

<h1>vmlinux ubd0=arch_rootfs1  mem=256M eth0=vde,/tmp/switch1</h1>

<h1>启动一台双网卡的uml，已经与之前定义的switch绑定</h1>

<h1>vmlinux ubd0=arch_rootfs2  mem=256M eth0=vde,/tmp/switch1 eth1=vde,/tmp/switch1</h1>

<p>{% endcodeblock %}</p>

<!-- more -->


<p>启动完成后，在linux里用常规的网络设置方法完成ip地址的分配。
{% codeblock network configuration lang:bash %}</p>

<h1>ifconfig eth0 192.168.0.101 up</h1>

<h1>ifconfig eth1 192.168.0.101 up</h1>

<h1>确定host上的tap0已经正确设置并启用</h1>

<h1>ifconfig tap0 192.168.0.100 up</h1>

<p>{% endcodeblock %}
完成后用ping测试下，各个uml和host之间的网络通信是否正常。</p>

<p>接下来设置uml与主机的文件系统共享。
{% codeblock mounting hostfs directory lang:bash %}</p>

<h1>mkdir -p /root/source</h1>

<h1>mount none /root/source/ -t hostfs -o /home/yourname/code/</h1>

<p>{% endcodeblock %}</p>

<p>完成后，应该可以读写hostfs的文件，测试一下，放一份linux源代码在host的<code>/home/yourname/code/</code>下，
目录结构是<code>/home/yourname/code/linux</code>，自己写的module的目录在<code>/home/yourname/code/my_module</code>，
对应在uml上的目录就是<code>/root/source/linux</code>和<code>/root/source/my_module</code>。
{% codeblock Makefile of my_module :bash %}
obj-m := test.o #source file is test.c
KDIR := /root/source/linux
PWD := $(shell pwd)
default:</p>

<pre><code>    $(MAKE) -C $(KDIR) SUBDIRS=$(PWD) modules
</code></pre>

<p>{% endcodeblock %}</p>

<p>在<code>/root/source/my_module</code>下<code>make</code>就可以了，在<code>#insmod test.ko</code>测试，如果crash了就<code>kill</code>掉host上的<code>vmlinux</code>进程，非常方便快速。
{% codeblock kill all vmlinux process lang:bash %}</p>

<h1>killall vmlinux processes if uml can't be started</h1>

<h1>killall vmlinux</h1>

<p>{% endcodeblock %}</p>

<p>当然，gdb也是可以用的，可以参考uml gdb调试相关的介绍。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在SUSE Enterprise上安装Systemtap，解决build-id mismatch]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/08/16/install-systemtap-on-suse/"/>
    <updated>2012-08-16T20:49:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/08/16/install-systemtap-on-suse</id>
    <content type="html"><![CDATA[<p>最近做LVS的性能测试，自然会用的各种工具检视系统状态，用了很多工具，都
不甚满意，于是想尝试下Systemtap，系统环境是SUSE Enterprise SP2,已经3.0的
kernel，可以search到的suse上安装systemtap的内容不多，碰到问题并解决了
的更是少，自然我又要成为苦逼的问题解决者了。</p>

<h2>正常安装的步骤是</h2>

<ul>
<li>添加suse enterprise的包含debuginfo的iso，有几个G啊</li>
<li><code>zypper in kernel-default-debuginfo</code></li>
<li><code>zypper in systemtap</code></li>
</ul>


<p>当我做完了以上这些，运行一个简单的脚本时:</p>

<p>{% codeblock systemtap hello_world lang:bash %}
sudo stap -ve 'probe begin { log("hello world") exit() }'
{% endcodeblock  %}</p>

<!-- more -->


<p>这个简单的例子是ok的，因为它不会涉及到systemtap的pass5，与kernel
module交互。跑下面这个的时候，就不行了:
{% codeblock systemtap hello_world lang:bash %}
stap -v -e 'probe vfs.read {printf("has VFS read()\n"); exit()}'
{% endcodeblock  %}
会报出类似<code>ERROR: Build-id mismatch: "kernel" vs. "vmlinux" byte 0 (0xf9 vs 0xa2)</code>的问题，关于build-id mismatch这个问题，还是可以
搜索到不少资料的，不过告诉如何解决的也不多。先说明下这个问题，
<code>kernel-default-debuginfo</code>里面是附赠了一个kernel elf文件的，在SUSE里，
安装后放在<code>/usr/lib/debug/boot/vmlinux-3.0.13-0.27-default.debug</code>，这
是debug版本的kernel vmlinux文件，是elf原始文件，未经压缩。<code>stap</code>运行时会去检查它的。
执行<code>eu-readelf -n
/usr/lib/debug/boot/vmlinux-3.0.13-0.27-default.debug</code>，可以拿到的
build-id，执行结果的第一个字节是<code>f9</code>，那另外一个<code>a2</code>是从哪里得出来的，猜想应该和当
前运行的kernel有关，于是拿到当前运行的vmlinux的压缩文件，解压后检查
build-id：
{% codeblock check build-id lang:bash %}
   gunzip /boot/vmlinux-3.0.13-0.27-default.gz
   eu-readelf -n /boot/vmlinux-3.0.13-0.27-default
{% endcodeblock %}
执行结果也是<code>f9</code>，是对得上的啊。</p>

<p>但是注意，这份.gz文件是SUSE附赠的，跟当前跑的vmlinz文件没有关系，于是
开始打vmlinuz文件的主意，得先想办法把vmlinuz->vmlinux，及bzImage到elf，
才能去取它的build-id，
<a href="%E8%BF%99%E9%87%8C">https://www.globalways.net/blog/archives/76-Extracting-bzImage-from-vmlinuz.html</a>有可行的方法。
<code>eu-readelf -n vmlinux-unpacked</code>的结果果然是<code>a2</code>，我只能是怀疑SUSE给的
默认vmlinuz有问题了，不能和SUSE提供的用于debuginfo的vmlinux匹配。</p>

<p>只能另外想办法，其实systemtap做kernel mismatch判断的关键代码，就在:</p>

<p>{% codeblock systemtap/runtime/sym.c lang:c %}
static int <em>stp_build_id_check (struct </em>stp_module *m,</p>

<pre><code>        unsigned long notes_addr,
        struct task_struct *tsk)
</code></pre>

<p>{
...
/<em> just hack it </em>/
if (rc || (theory != practice)) {</p>

<pre><code>  const char *basename;
  basename = strrchr(m-&gt;path, '/');
  if (basename)
</code></pre>

<p>|   basename++;</p>

<pre><code>  else
</code></pre>

<p>|   basename = m->path;</p>

<h1>if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,27)</h1>

<pre><code>  _stp_error ("Build-id mismatch: \"%s\" vs. \"%s\" byte %d (0x%02x vs 0x%02x) address %#lx rc %d\n",
</code></pre>

<p>|   |     m->name, basename, j, theory, practice, notes_addr, rc);</p>

<pre><code>  return 1;
</code></pre>

<h1>else</h1>

<pre><code>  /* This branch is a surrogate for kernels affected by Fedora bug
   * #465873. */
  _stp_warn (KERN_WARNING
</code></pre>

<p>|   |    "Build-id mismatch: \"%s\" vs. \"%s\" byte %d (0x%02x vs 0x%02x) rc %d\n",
|   |    m->name, basename, j, theory, practice, rc);</p>

<h1>endif</h1>

<pre><code>  break;
} /* end mismatch */
</code></pre>

<p>{% endcodeblock %}</p>

<p>关键就在于<code>if (rc || (theory != practice))</code>，小小修改的一下，写成:
<code>if (0 &amp;&amp; (rc || (theory != practice)))</code>就OK了，绕过了检查。然后重新编
译systemtap，直接用的SUSE提供的老版本的源码，新的会有问题，注意还要用
到SUSE提供的<code>libdwfl</code>的source code，才能完成编译，编译时在
<code>/usr/src/packages/SOURCES/elfutils-0.152/libdwfl/linux-kernel-modules.c</code>
文件上遇到了问题，稍作修改就可过关。尝试下新编译出来的<code>stap</code>，完全OK，它是不知
道'build-id mismatch`为何物的，敢这样改也就是因为都是offical的东西，本
应该match得上，应该不会出什么大问题，实践中检验吧。</p>

<p>Have fun.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[升级Netxen网卡驱动及firmware]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/08/16/upgrade-the-netxen-nic/"/>
    <updated>2012-08-16T20:15:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/08/16/upgrade-the-netxen-nic</id>
    <content type="html"><![CDATA[<p>在公司做lvs测试用机是HP DL580 G7，除了网卡，其它的配置都不错，网卡是板
载的netxen的1G卡，还有几张netxen的10G卡，观察<code>/proc/interrupts</code>，每块
10G卡对应的中断都只有4个，而且没有识别成tx-0/rx-0的形式，不像支持
multiple queue，不禁使我对SUSE 11
Enterprise SP2的netxen驱动产生的怀疑。<code>dmesg</code>也可以发现有抱怨driver和
firmware的比较旧，于是萌生了升级驱动的想法。</p>

<!-- more -->


<p>用<code>netxen</code>和<code>nx3031</code>search
了很久而不得，偶然通过<code>nx_nic</code>找到了HP网卡的驱动
<a href="%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80">http://h20000.www2.hp.com/bizsupport/TechSupport/SoftwareIndex.jsp?lang=en&amp;cc=us&amp;prodNameId=3913538&amp;prodTypeId=329290&amp;prodSeriesId=3913537&amp;swLang=8&amp;taskId=135&amp;swEnvOID=4049</a>，包括firmware的自动升级程序和网卡驱动。值得吐一把嘈的是，HP
网站上driver是老旧的，不支持稍新的内核，貌似只支持到2.6.20以前的，编译
时会报<code>net_dev struct</code>相关的错，费了
那么大的力气，搞来个不能用的，之后还是google到了最新的驱动，在一个连域
名都没有的
<a href="ftp">ftp://15.217.49.75/ftp2/pub/softlib2/software1/pubsw-linux/p280489831/</a>
上，haha，真欢乐，HP sucks.按照说明先升级好driver，<code>modprobe nx_nic</code>后，
就可以升级firmware了，要注意的是，把每个网卡都up起来，才能保证所有都升
级成功，我拆掉了一些bond，保证以前是slave状态的卡都up起来之后，经过10
分钟左右，升级成功。重启后<code>dmesg</code>里的complains少了很多，新版本也显示出
来了，还是没识别出multiple queue，编译驱动时也检查了选项，确实没有。同
事也咨询过HP的sales，也不知道多队列网卡为何物，与Intel的资料的专业性相比，我对
HP的网卡实在是无力吐嘈。之后研究下再驱动代码吧，有代码有真相。</p>

<p>Have fun.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP状态迁移及状态码]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/08/09/check-tcp-connections/"/>
    <updated>2012-08-09T11:39:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/08/09/check-tcp-connections</id>
    <content type="html"><![CDATA[<p>{% imgpopup /images/post_img/Tcp_state_diagram.svg  50%  %}</p>

<p>想必大家对上图都比较熟悉了，补充下内核里对以上状态码的表示，顺便对源代码做了下改动，每个
状态的代码都补出来了。</p>

<p>{% codeblock tcp_state.h lang:c %}</p>

<p>enum {</p>

<pre><code>TCP_ESTABLISHED = 1,
TCP_SYN_SENT    = 2,
TCP_SYN_RECV    = 3,
TCP_FIN_WAIT1   = 4,
TCP_FIN_WAIT2   = 5,
TCP_TIME_WAIT   = 6
TCP_CLOSE       = 7, 
TCP_CLOSE_WAIT  = 8,
TCP_LAST_ACK    = 9,
TCP_LISTEN      = 10,
TCP_CLOSING     = 11,   /* Now a valid state */

TCP_MAX_STATES  /* Leave at the end! */
</code></pre>

<p>};</p>

<p>{% endcodeblock   %}</p>

<!-- more -->


<p>用<code>cat /proc/net/tcp</code>打印时，输出所有active connections，
<a href="%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E4%BB%8B%E7%BB%8D">http://search.cpan.org/~salva/Linux-Proc-Net-TCP-0.03/lib/Linux/Proc/Net/TCP.pm#The_/proc/net/tcp_documentation</a>。</p>

<p><code>st</code>这一列即代表连接状态，下面该怎么做，你懂的。</p>

<p>要想快速高效的显示连接状态信息，推荐用(ss)[http://stackoverflow.com/questions/11763376/difference-between-netstat-and-ss-in-linux]。
Have fun.</p>
]]></content>
  </entry>
  
</feed>
