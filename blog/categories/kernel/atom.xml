<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel | Code Mind]]></title>
  <link href="http://gekben.gitcd.com/blog/categories/kernel/atom.xml" rel="self"/>
  <link href="http://gekben.gitcd.com/"/>
  <updated>2013-03-19T08:31:40+08:00</updated>
  <id>http://gekben.gitcd.com/</id>
  <author>
    <name><![CDATA[Kevin Zeng]]></name>
    <email><![CDATA[kevintech08@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[基于lvs的全透明传输思考]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/11/13/full-transparent-with-lvs/"/>
    <updated>2012-11-13T17:28:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/11/13/full-transparent-with-lvs</id>
    <content type="html"><![CDATA[<p>互联网公司一般使用LB的方式，是把Web App Server作为real server，最简单的情况是请求在这里终结，响应由Web App Server返回，一般web server不用再访问外网了。再复杂的情况，就
是Web App Server又作为client去请求内网的其它资源，如DB，其它Web Server等，在这种典型的互联网部署模式下，一般可以把Web Server看作终点。</p>

<p>还有一种部署方式，load balancing的对象不是Web App Server，而是firewall系统，比如用linux搭建的firewall，经过firewall的流量需要全透明传输，即不修改layer 2以上的信息，firewall
对过滤后合格的包做forwarding。在这种部署模式下，LB也需要有全透明传输的能力，画个图解释一下:</p>

<p>{% imgpopup /images/post_img/lvs_trans_firewall.png 70%  %}</p>

<!-- more -->


<p>这里所说的LB都是lvs，以http为例，首先client请求WWW Server，client有自己的公网地址，组装好的ip header，source ip是cip，destination ip是www_ip，
source port是cport，destination port 80，lvs要支持这种destination ip非vip的部署，需要使用到fwmark。
首先是要mark出这个请求：
{% codeblock mark the traffic lang:bash %}</p>

<h1>iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-xmark 1</h1>

<p>{% endcodeblock %}</p>

<p>然后交由lvs处理：
{% codeblock direct traffic to local_in hook lang:bash %}</p>

<h1>ip rule add prio 101 fwmark 1 table 101</h1>

<h1>ip route add local 0/0 dev lo table 10</h1>

<p>{% endcodeblock %}</p>

<p>lvs的fwmark service及real server：
{% codeblock define lvs fwmark service and real servers hook:bash %}</p>

<h1>ipvsadm -A -f 1 -p #default scheduler is wlc,enable persistent</h1>

<h1>ipvsadm -a -f 1 -r rs1 #default xmit method is direct routing</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<p>{% endcodeblock %}</p>

<p>这样配置后，LB1充当了透明的load balancer，经过LB1的request的layer2以上的信息都没有变化，rs1~rs4都可以接收请求，
并进行过滤处理，也是全透明的。先考虑没有LB2的情况，request从rs出来，被转发到Router，Router在将request转发至外网，
直至WWW_Server。从Client到WWW_server，layer2以上的地址和端口信息都没有改变。</p>

<p>Response从WWW_Server出发，source是www_ip：80，destination是cip:cport，发向Router，Router这时候不知道该
如何进行路由了，因为从rs1出来的request，对应的response也要回到rs1，Router没有rs1的内部ip，也没有连接信息
参考，response回不到对应real server，连接无法建立起来。</p>

<p>产生这个问题的原因是real server是有状态的，在request经过它时做了记录，例如linux的防火墙的基础conn_track，它期待response
也必须经过这台real server，tcp/udp以及基于它们的应用层协议都是如此。为了让response能回到正确的real server，需要利用到一种技术，
所有request和response都要经过它转发，在request进来时，记录它的源mac地址和接收包的device，response回来时，绕过路由直接发送给
之前记录的源mac，维护连接表+mac地址是必须的。</p>

<p>目前支持这种技术的设备有F5，被称为Auto Last Hop，还有Juniper的某些设备，称作Reverse Route。把这种设备部署在LB2的位置，即可实现
full transparent，当然，也不是一定要单独搞一台LB2，用一台LB既做Load Balancer又做反向路由也是可以的。使用这种设备的最大缺点就是，
贵。于是想基于Lvs实现方向路由功能，想想也是非常合理的，lvs本身可以很好的维护管理连接状态，它有一张connection table，连接状态迁移，
超时机制，垃圾回收都已经非常成熟，再加上source mac和in_dev并不难。需要注意的是，lvs在这种使用模式下，应该定义fwmark service，它的rs只有一个，
就是下一跳的ip。经过一段时间的开发，对lvs进行修改后，基于lvs的反向路由功能已经OK，准备下一篇来讲讲细节。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建基于User Mode Linux网络模块开发环境]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/11/13/build-uml-development-environment/"/>
    <updated>2012-11-13T15:40:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/11/13/build-uml-development-environment</id>
    <content type="html"><![CDATA[<p>最近在写一些内核网络模块，需要一个好用的编译、调试、测试环境，选用了uml，记录一下设置多台uml和host通信，并share fs。
uml基本安装和使用可以参考<a href="https://wiki.archlinux.org/index.php/User-mode_Linux#Setup_by_rootfs_.2B_tap">archlinux uml</a>。
{% codeblock start uml with vde support lang:bash %}</p>

<h1>先启动虚拟switch</h1>

<h1>vde_switch -s /tmp/switch1 -tap tap0 -m 666</h1>

<h1>启动一台单网卡的uml，已经与之前定义的switch绑定</h1>

<h1>vmlinux ubd0=arch_rootfs1  mem=256M eth0=vde,/tmp/switch1</h1>

<h1>启动一台双网卡的uml，已经与之前定义的switch绑定</h1>

<h1>vmlinux ubd0=arch_rootfs2  mem=256M eth0=vde,/tmp/switch1 eth1=vde,/tmp/switch1</h1>

<p>{% endcodeblock %}</p>

<!-- more -->


<p>启动完成后，在linux里用常规的网络设置方法完成ip地址的分配。
{% codeblock network configuration lang:bash %}</p>

<h1>ifconfig eth0 192.168.0.101 up</h1>

<h1>ifconfig eth1 192.168.0.101 up</h1>

<h1>确定host上的tap0已经正确设置并启用</h1>

<h1>ifconfig tap0 192.168.0.100 up</h1>

<p>{% endcodeblock %}
完成后用ping测试下，各个uml和host之间的网络通信是否正常。</p>

<p>接下来设置uml与主机的文件系统共享。
{% codeblock mounting hostfs directory lang:bash %}</p>

<h1>mkdir -p /root/source</h1>

<h1>mount none /root/source/ -t hostfs -o /home/yourname/code/</h1>

<p>{% endcodeblock %}</p>

<p>完成后，应该可以读写hostfs的文件，测试一下，放一份linux源代码在host的<code>/home/yourname/code/</code>下，
目录结构是<code>/home/yourname/code/linux</code>，自己写的module的目录在<code>/home/yourname/code/my_module</code>，
对应在uml上的目录就是<code>/root/source/linux</code>和<code>/root/source/my_module</code>。
{% codeblock Makefile of my_module :bash %}
obj-m := test.o #source file is test.c
KDIR := /root/source/linux
PWD := $(shell pwd)
default:</p>

<pre><code>    $(MAKE) -C $(KDIR) SUBDIRS=$(PWD) modules
</code></pre>

<p>{% endcodeblock %}</p>

<p>在<code>/root/source/my_module</code>下<code>make</code>就可以了，在<code>#insmod test.ko</code>测试，如果crash了就<code>kill</code>掉host上的<code>vmlinux</code>进程，非常方便快速。
{% codeblock kill all vmlinux process lang:bash %}</p>

<h1>killall vmlinux processes if uml can't be started</h1>

<h1>killall vmlinux</h1>

<p>{% endcodeblock %}</p>

<p>当然，gdb也是可以用的，可以参考uml gdb调试相关的介绍。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在SUSE Enterprise上安装Systemtap，解决build-id mismatch]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/08/16/install-systemtap-on-suse/"/>
    <updated>2012-08-16T20:49:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/08/16/install-systemtap-on-suse</id>
    <content type="html"><![CDATA[<p>最近做LVS的性能测试，自然会用的各种工具检视系统状态，用了很多工具，都
不甚满意，于是想尝试下Systemtap，系统环境是SUSE Enterprise SP2,已经3.0的
kernel，可以search到的suse上安装systemtap的内容不多，碰到问题并解决了
的更是少，自然我又要成为苦逼的问题解决者了。</p>

<h2>正常安装的步骤是</h2>

<ul>
<li>添加suse enterprise的包含debuginfo的iso，有几个G啊</li>
<li><code>zypper in kernel-default-debuginfo</code></li>
<li><code>zypper in systemtap</code></li>
</ul>


<p>当我做完了以上这些，运行一个简单的脚本时:</p>

<p>{% codeblock systemtap hello_world lang:bash %}
sudo stap -ve 'probe begin { log("hello world") exit() }'
{% endcodeblock  %}</p>

<!-- more -->


<p>这个简单的例子是ok的，因为它不会涉及到systemtap的pass5，与kernel
module交互。跑下面这个的时候，就不行了:
{% codeblock systemtap hello_world lang:bash %}
stap -v -e 'probe vfs.read {printf("has VFS read()\n"); exit()}'
{% endcodeblock  %}
会报出类似<code>ERROR: Build-id mismatch: "kernel" vs. "vmlinux" byte 0 (0xf9 vs 0xa2)</code>的问题，关于build-id mismatch这个问题，还是可以
搜索到不少资料的，不过告诉如何解决的也不多。先说明下这个问题，
<code>kernel-default-debuginfo</code>里面是附赠了一个kernel elf文件的，在SUSE里，
安装后放在<code>/usr/lib/debug/boot/vmlinux-3.0.13-0.27-default.debug</code>，这
是debug版本的kernel vmlinux文件，是elf原始文件，未经压缩。<code>stap</code>运行时会去检查它的。
执行<code>eu-readelf -n
/usr/lib/debug/boot/vmlinux-3.0.13-0.27-default.debug</code>，可以拿到的
build-id，执行结果的第一个字节是<code>f9</code>，那另外一个<code>a2</code>是从哪里得出来的，猜想应该和当
前运行的kernel有关，于是拿到当前运行的vmlinux的压缩文件，解压后检查
build-id：
{% codeblock check build-id lang:bash %}
   gunzip /boot/vmlinux-3.0.13-0.27-default.gz
   eu-readelf -n /boot/vmlinux-3.0.13-0.27-default
{% endcodeblock %}
执行结果也是<code>f9</code>，是对得上的啊。</p>

<p>但是注意，这份.gz文件是SUSE附赠的，跟当前跑的vmlinz文件没有关系，于是
开始打vmlinuz文件的主意，得先想办法把vmlinuz->vmlinux，及bzImage到elf，
才能去取它的build-id，
<a href="%E8%BF%99%E9%87%8C">https://www.globalways.net/blog/archives/76-Extracting-bzImage-from-vmlinuz.html</a>有可行的方法。
<code>eu-readelf -n vmlinux-unpacked</code>的结果果然是<code>a2</code>，我只能是怀疑SUSE给的
默认vmlinuz有问题了，不能和SUSE提供的用于debuginfo的vmlinux匹配。</p>

<p>只能另外想办法，其实systemtap做kernel mismatch判断的关键代码，就在:</p>

<p>{% codeblock systemtap/runtime/sym.c lang:c %}
static int <em>stp_build_id_check (struct </em>stp_module *m,</p>

<pre><code>        unsigned long notes_addr,
        struct task_struct *tsk)
</code></pre>

<p>{
...
/<em> just hack it </em>/
if (rc || (theory != practice)) {</p>

<pre><code>  const char *basename;
  basename = strrchr(m-&gt;path, '/');
  if (basename)
</code></pre>

<p>|   basename++;</p>

<pre><code>  else
</code></pre>

<p>|   basename = m->path;</p>

<h1>if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,27)</h1>

<pre><code>  _stp_error ("Build-id mismatch: \"%s\" vs. \"%s\" byte %d (0x%02x vs 0x%02x) address %#lx rc %d\n",
</code></pre>

<p>|   |     m->name, basename, j, theory, practice, notes_addr, rc);</p>

<pre><code>  return 1;
</code></pre>

<h1>else</h1>

<pre><code>  /* This branch is a surrogate for kernels affected by Fedora bug
   * #465873. */
  _stp_warn (KERN_WARNING
</code></pre>

<p>|   |    "Build-id mismatch: \"%s\" vs. \"%s\" byte %d (0x%02x vs 0x%02x) rc %d\n",
|   |    m->name, basename, j, theory, practice, rc);</p>

<h1>endif</h1>

<pre><code>  break;
} /* end mismatch */
</code></pre>

<p>{% endcodeblock %}</p>

<p>关键就在于<code>if (rc || (theory != practice))</code>，小小修改的一下，写成:
<code>if (0 &amp;&amp; (rc || (theory != practice)))</code>就OK了，绕过了检查。然后重新编
译systemtap，直接用的SUSE提供的老版本的源码，新的会有问题，注意还要用
到SUSE提供的<code>libdwfl</code>的source code，才能完成编译，编译时在
<code>/usr/src/packages/SOURCES/elfutils-0.152/libdwfl/linux-kernel-modules.c</code>
文件上遇到了问题，稍作修改就可过关。尝试下新编译出来的<code>stap</code>，完全OK，它是不知
道'build-id mismatch`为何物的，敢这样改也就是因为都是offical的东西，本
应该match得上，应该不会出什么大问题，实践中检验吧。</p>

<p>Have fun.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[了解lvs调试]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/07/26/lvs-debug-howto/"/>
    <updated>2012-07-26T21:32:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/07/26/lvs-debug-howto</id>
    <content type="html"><![CDATA[<p>虽然对lvs的实现代码也算是心里有数了，但遇到一些具体问题时还拿不准，这时候就想到了lvs提供的调试功能，
是内核提供的一个选项。一般发行版默认是没打开的，至少我接触的suse enterprice和archlinux都是关闭
了ip_vs_debug，一起来看看怎么启用吧。
{% codeblock kernel config  %}
CONFIG_IP_VS_DEBUG=y
{% endcodeblock %}</p>

<p>在默认的config文件里找到这一项，可能是注释状态，改好后，就可以开始编译内核了，这个选项不是m，必须重新
编译产生新内核，在用<code>mkinitcpio -k kernel_full_name -c config</code>或者<code>mkinitrd -k kernel_full_name
</code>，前面是针对archlinux的，后面是针对suse的，这里有坑，此处略去1000字，反正记得如果用新内核启动，出现
找不到root的情况，八成是initrd做得有问题，fs相关的modules的问题。</p>

<p>新内核一切安好后，就可以玩lvs的调试功能了。先运行下<code>ipvsadm</code>，再<code>lsmod</code>看下<code>ip_vs</code>是否已经加载了。
再后面，就可以检查期待已久的调试选项了
{% codeblock ip_vs debug in /proc %}
/proc/sys/net/ipv4/vs/debug_level
{% endcodeblock %}</p>

<p><code>cat debug_level</code>一下，默认值是0，以后可以通过sysctl来更改默认值，下面讲下这个值的含义:
{% codeblock ip_vs_core.c lang:c %}
 IP_VS_DBG_BUF(1, "Forward ICMP: failed checksum from %s!\n", IP_VS_DBG_ADDR(af, snet));
{% endcodeblock %}
上面是一个debug macro的调用的例子，下面是宏的具体定义：</p>

<p>{% codeblock ip_vs.h lang:c %}</p>

<h1>ifdef CONFIG_IP_VS_DEBUG</h1>

<p>extern int ip_vs_get_debug_level(void);</p>

<h1>define IP_VS_DBG(level, msg...)            \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level())   \
        printk(KERN_DEBUG "IPVS: " msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_RL(msg...)                \</h1>

<pre><code>do {                        \
    if (net_ratelimit())            \
        printk(KERN_DEBUG "IPVS: " msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_PKT(level, pp, skb, ofs, msg)     \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level())   \
    pp-&gt;debug_packet(pp, skb, ofs, msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_RL_PKT(level, pp, skb, ofs, msg)  \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level() &amp;&amp; \
    net_ratelimit())            \
    pp-&gt;debug_packet(pp, skb, ofs, msg);    \
} while (0)
</code></pre>

<h1>else   /<em> NO DEBUGGING at ALL </em>/</h1>

<h1>define IP_VS_DBG(level, msg...)  do {} while (0)</h1>

<h1>define IP_VS_DBG_RL(msg...)  do {} while (0)</h1>

<h1>define IP_VS_DBG_PKT(level, pp, skb, ofs, msg)     do {} while (0)</h1>

<h1>define IP_VS_DBG_RL_PKT(level, pp, skb, ofs, msg)  do {} while (0)</h1>

<h1>endif</h1>

<p>{% endcodeblock %}
可以清晰的看到，打印debug日志的条件是<code>level &lt;= ip_vs_get_debug_level()</code>，这个函数调用的返回值就是
<code>/proc/sys/net/ipv4/vs/debug_level</code>，万物皆文件的理念又体现了。
可见level的值越小，其优先级越高，查了一遍ip_vs的相关代码，level最大值不过12，也就是说，
只要把<code>/proc/sys/net/ipv4/vs/debug_level</code>设置成12，就能保证所有ip_vs的日志输出了，但一般没那个必要。
{% codeblock set debug_level to 12 %}
echo 12 > /proc/sys/net/ipv4/vs/debug_level
{% endcodeblock %}
对了，日志输出是在dmesg。</p>

<p>好了，方法就差不多是这些了，接下来就边看代码，边测试，边看输出吧。
have fun.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[学习万兆以太网，硬件和优化方法]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/07/15/hardware-and-10gb-network/"/>
    <updated>2012-07-15T11:34:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/07/15/hardware-and-10gb-network</id>
    <content type="html"><![CDATA[<p>上周末去杭州参加了<a href="http://adc.taobao.com/">ADC</a>，收获不小，关注的几个
topic都和万兆网有关。最近也在做用LVS替换F5的调研，对性能的要求非常高，
于是开始关注起10Gbe，记录一些资料和心得。</p>

<h2>个人关注点</h2>

<ul>
<li><a href="https://speakerdeck.com/u/gekben/p/by-intel">拥抱万兆以太网时代 - 探索英特尔® 至强® 处理器平台上的应用性能优化</a>

<ul>
<li>图片缩放及gzip的增强</li>
<li>E5-2600 DDIO这个绝对是重点，对网络收发性能提升明显</li>
<li>性价比高的10G BASE-T会逐渐成为主流</li>
</ul>
</li>
<li><a href="https://speakerdeck.com/u/gekben/p/lvs-used-in-taobao">LVS在淘宝环境中的应用</a>

<ul>
<li>FULLNAT模式，对大型网络降低运维难度非常好，但对LVS的改造很大，增加了复杂性</li>
<li>SynProxy，在lvs上实现syn cookie防御机制，不明白为什么要做在这里</li>
<li>taobao也没用上sandy bridge，还是E5640+intel 82599万兆卡</li>
<li>多队列网卡，每个core绑定网卡的一个队列</li>
<li>KeepAlived增强，select->epoll,localaddr</li>
<li>Cluster，因为real server本身无状态，水平扩容好做，OSPF</li>
<li>4/7层结合，lvs+nginx，nginx做7层业务负载</li>
</ul>
</li>
</ul>


<h2>硬件</h2>

<ul>
<li><a href="http://storage.chinabyte.com/447/12275447.shtml">介绍</a>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Sandy_Bridge_%28microarchitecture%29">sandy bridge</a> cpu列表</li>
<li><a href="http://www.intel.com/content/www/us/en/network-adapters/gigabit-network-adapters/ethernet-x520.html">x520</a></li>
<li><a href="http://ark.intel.com/products/58954/Intel-Ethernet-Converged-Network-Adapter-X540-T2">x540</a></li>
<li><a href="http://www.emulex.com/products/10gbe-network-adapters-nic/emulex-branded/oce11102-nt/overview.html">emulex 手头正好有这块卡</a></li>
</ul>
</li>
</ul>


<h2>优化</h2>

<ul>
<li>intel那个slides里有几个建议

<ul>
<li>Process affinity - socket</li>
<li>Interrupts affinity - socket</li>
<li>Disable LLDPAD,IPTABLES,IP6TABLES,SELINUX,IRQBALANCE</li>
</ul>
</li>
<li><a href="http://timetobleed.com/useful-kernel-and-driver-performance-tweaks-for-your-linux-server/">Linux kernel and driver</a></li>
<li><a href="http://www.linuxfoundation.org/collaborate/workgroups/networking">GRO,GSO,TSO...</a></li>
<li>Papers

<ul>
<li><a href="http://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=10Gbe+linux+optimization&amp;source=web&amp;cd=1&amp;ved=0CFQQFjAA&amp;url=http%3A%2F%2Fdocs.redhat.com%2Fdocs%2Fen-US%2FRed_Hat_Enterprise_Linux%2F6%2Fpdf%2FPerformance_Tuning_Guide%2FRed_Hat_Enterprise_Linux-6-Performance_Tuning_Guide-en-US.pdf&amp;ei=3D4RUKmWJ6O3iQed2YHYCw&amp;usg=AFQjCNGRhggpqYuUP0LQwoQE1uofWlN2dQ">RedHat</a></li>
<li><a href="http://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=10Gbe+linux+optimization&amp;source=web&amp;cd=5&amp;ved=0CGUQFjAE&amp;url=http%3A%2F%2Fakashi.ci.i.u-tokyo.ac.jp%2Flab%2Fcmsdesigner%2Fdlfile.php%3Fentryname%3Dpublic%26entryid%3D00096%26fileid%3D00000001%26%2Fysn-camera.pdf&amp;ei=3D4RUKmWJ6O3iQed2YHYCw&amp;usg=AFQjCNGXO4wm2DQ8A6cAg1IsVzjbGjzvDg">Google</a></li>
<li><a href="http://www.kernel.org/doc/ols/2009/ols2009-pages-169-184.pdf">IBM</a></li>
</ul>
</li>
</ul>


<p>先记录到这里，边实践边补充。</p>
]]></content>
  </entry>
  
</feed>
