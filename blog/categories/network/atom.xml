<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: network | Code Mind]]></title>
  <link href="http://gekben.gitcd.com/blog/categories/network/atom.xml" rel="self"/>
  <link href="http://gekben.gitcd.com/"/>
  <updated>2013-03-19T08:31:40+08:00</updated>
  <id>http://gekben.gitcd.com/</id>
  <author>
    <name><![CDATA[Kevin Zeng]]></name>
    <email><![CDATA[kevintech08@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[基于lvs的全透明传输思考]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/11/13/full-transparent-with-lvs/"/>
    <updated>2012-11-13T17:28:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/11/13/full-transparent-with-lvs</id>
    <content type="html"><![CDATA[<p>互联网公司一般使用LB的方式，是把Web App Server作为real server，最简单的情况是请求在这里终结，响应由Web App Server返回，一般web server不用再访问外网了。再复杂的情况，就
是Web App Server又作为client去请求内网的其它资源，如DB，其它Web Server等，在这种典型的互联网部署模式下，一般可以把Web Server看作终点。</p>

<p>还有一种部署方式，load balancing的对象不是Web App Server，而是firewall系统，比如用linux搭建的firewall，经过firewall的流量需要全透明传输，即不修改layer 2以上的信息，firewall
对过滤后合格的包做forwarding。在这种部署模式下，LB也需要有全透明传输的能力，画个图解释一下:</p>

<p>{% imgpopup /images/post_img/lvs_trans_firewall.png 70%  %}</p>

<!-- more -->


<p>这里所说的LB都是lvs，以http为例，首先client请求WWW Server，client有自己的公网地址，组装好的ip header，source ip是cip，destination ip是www_ip，
source port是cport，destination port 80，lvs要支持这种destination ip非vip的部署，需要使用到fwmark。
首先是要mark出这个请求：
{% codeblock mark the traffic lang:bash %}</p>

<h1>iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-xmark 1</h1>

<p>{% endcodeblock %}</p>

<p>然后交由lvs处理：
{% codeblock direct traffic to local_in hook lang:bash %}</p>

<h1>ip rule add prio 101 fwmark 1 table 101</h1>

<h1>ip route add local 0/0 dev lo table 10</h1>

<p>{% endcodeblock %}</p>

<p>lvs的fwmark service及real server：
{% codeblock define lvs fwmark service and real servers hook:bash %}</p>

<h1>ipvsadm -A -f 1 -p #default scheduler is wlc,enable persistent</h1>

<h1>ipvsadm -a -f 1 -r rs1 #default xmit method is direct routing</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<h1>ipvsadm -a -f 1 -r rs2</h1>

<p>{% endcodeblock %}</p>

<p>这样配置后，LB1充当了透明的load balancer，经过LB1的request的layer2以上的信息都没有变化，rs1~rs4都可以接收请求，
并进行过滤处理，也是全透明的。先考虑没有LB2的情况，request从rs出来，被转发到Router，Router在将request转发至外网，
直至WWW_Server。从Client到WWW_server，layer2以上的地址和端口信息都没有改变。</p>

<p>Response从WWW_Server出发，source是www_ip：80，destination是cip:cport，发向Router，Router这时候不知道该
如何进行路由了，因为从rs1出来的request，对应的response也要回到rs1，Router没有rs1的内部ip，也没有连接信息
参考，response回不到对应real server，连接无法建立起来。</p>

<p>产生这个问题的原因是real server是有状态的，在request经过它时做了记录，例如linux的防火墙的基础conn_track，它期待response
也必须经过这台real server，tcp/udp以及基于它们的应用层协议都是如此。为了让response能回到正确的real server，需要利用到一种技术，
所有request和response都要经过它转发，在request进来时，记录它的源mac地址和接收包的device，response回来时，绕过路由直接发送给
之前记录的源mac，维护连接表+mac地址是必须的。</p>

<p>目前支持这种技术的设备有F5，被称为Auto Last Hop，还有Juniper的某些设备，称作Reverse Route。把这种设备部署在LB2的位置，即可实现
full transparent，当然，也不是一定要单独搞一台LB2，用一台LB既做Load Balancer又做反向路由也是可以的。使用这种设备的最大缺点就是，
贵。于是想基于Lvs实现方向路由功能，想想也是非常合理的，lvs本身可以很好的维护管理连接状态，它有一张connection table，连接状态迁移，
超时机制，垃圾回收都已经非常成熟，再加上source mac和in_dev并不难。需要注意的是，lvs在这种使用模式下，应该定义fwmark service，它的rs只有一个，
就是下一跳的ip。经过一段时间的开发，对lvs进行修改后，基于lvs的反向路由功能已经OK，准备下一篇来讲讲细节。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建基于User Mode Linux网络模块开发环境]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/11/13/build-uml-development-environment/"/>
    <updated>2012-11-13T15:40:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/11/13/build-uml-development-environment</id>
    <content type="html"><![CDATA[<p>最近在写一些内核网络模块，需要一个好用的编译、调试、测试环境，选用了uml，记录一下设置多台uml和host通信，并share fs。
uml基本安装和使用可以参考<a href="https://wiki.archlinux.org/index.php/User-mode_Linux#Setup_by_rootfs_.2B_tap">archlinux uml</a>。
{% codeblock start uml with vde support lang:bash %}</p>

<h1>先启动虚拟switch</h1>

<h1>vde_switch -s /tmp/switch1 -tap tap0 -m 666</h1>

<h1>启动一台单网卡的uml，已经与之前定义的switch绑定</h1>

<h1>vmlinux ubd0=arch_rootfs1  mem=256M eth0=vde,/tmp/switch1</h1>

<h1>启动一台双网卡的uml，已经与之前定义的switch绑定</h1>

<h1>vmlinux ubd0=arch_rootfs2  mem=256M eth0=vde,/tmp/switch1 eth1=vde,/tmp/switch1</h1>

<p>{% endcodeblock %}</p>

<!-- more -->


<p>启动完成后，在linux里用常规的网络设置方法完成ip地址的分配。
{% codeblock network configuration lang:bash %}</p>

<h1>ifconfig eth0 192.168.0.101 up</h1>

<h1>ifconfig eth1 192.168.0.101 up</h1>

<h1>确定host上的tap0已经正确设置并启用</h1>

<h1>ifconfig tap0 192.168.0.100 up</h1>

<p>{% endcodeblock %}
完成后用ping测试下，各个uml和host之间的网络通信是否正常。</p>

<p>接下来设置uml与主机的文件系统共享。
{% codeblock mounting hostfs directory lang:bash %}</p>

<h1>mkdir -p /root/source</h1>

<h1>mount none /root/source/ -t hostfs -o /home/yourname/code/</h1>

<p>{% endcodeblock %}</p>

<p>完成后，应该可以读写hostfs的文件，测试一下，放一份linux源代码在host的<code>/home/yourname/code/</code>下，
目录结构是<code>/home/yourname/code/linux</code>，自己写的module的目录在<code>/home/yourname/code/my_module</code>，
对应在uml上的目录就是<code>/root/source/linux</code>和<code>/root/source/my_module</code>。
{% codeblock Makefile of my_module :bash %}
obj-m := test.o #source file is test.c
KDIR := /root/source/linux
PWD := $(shell pwd)
default:</p>

<pre><code>    $(MAKE) -C $(KDIR) SUBDIRS=$(PWD) modules
</code></pre>

<p>{% endcodeblock %}</p>

<p>在<code>/root/source/my_module</code>下<code>make</code>就可以了，在<code>#insmod test.ko</code>测试，如果crash了就<code>kill</code>掉host上的<code>vmlinux</code>进程，非常方便快速。
{% codeblock kill all vmlinux process lang:bash %}</p>

<h1>killall vmlinux processes if uml can't be started</h1>

<h1>killall vmlinux</h1>

<p>{% endcodeblock %}</p>

<p>当然，gdb也是可以用的，可以参考uml gdb调试相关的介绍。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[升级Netxen网卡驱动及firmware]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/08/16/upgrade-the-netxen-nic/"/>
    <updated>2012-08-16T20:15:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/08/16/upgrade-the-netxen-nic</id>
    <content type="html"><![CDATA[<p>在公司做lvs测试用机是HP DL580 G7，除了网卡，其它的配置都不错，网卡是板
载的netxen的1G卡，还有几张netxen的10G卡，观察<code>/proc/interrupts</code>，每块
10G卡对应的中断都只有4个，而且没有识别成tx-0/rx-0的形式，不像支持
multiple queue，不禁使我对SUSE 11
Enterprise SP2的netxen驱动产生的怀疑。<code>dmesg</code>也可以发现有抱怨driver和
firmware的比较旧，于是萌生了升级驱动的想法。</p>

<!-- more -->


<p>用<code>netxen</code>和<code>nx3031</code>search
了很久而不得，偶然通过<code>nx_nic</code>找到了HP网卡的驱动
<a href="%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80">http://h20000.www2.hp.com/bizsupport/TechSupport/SoftwareIndex.jsp?lang=en&amp;cc=us&amp;prodNameId=3913538&amp;prodTypeId=329290&amp;prodSeriesId=3913537&amp;swLang=8&amp;taskId=135&amp;swEnvOID=4049</a>，包括firmware的自动升级程序和网卡驱动。值得吐一把嘈的是，HP
网站上driver是老旧的，不支持稍新的内核，貌似只支持到2.6.20以前的，编译
时会报<code>net_dev struct</code>相关的错，费了
那么大的力气，搞来个不能用的，之后还是google到了最新的驱动，在一个连域
名都没有的
<a href="ftp">ftp://15.217.49.75/ftp2/pub/softlib2/software1/pubsw-linux/p280489831/</a>
上，haha，真欢乐，HP sucks.按照说明先升级好driver，<code>modprobe nx_nic</code>后，
就可以升级firmware了，要注意的是，把每个网卡都up起来，才能保证所有都升
级成功，我拆掉了一些bond，保证以前是slave状态的卡都up起来之后，经过10
分钟左右，升级成功。重启后<code>dmesg</code>里的complains少了很多，新版本也显示出
来了，还是没识别出multiple queue，编译驱动时也检查了选项，确实没有。同
事也咨询过HP的sales，也不知道多队列网卡为何物，与Intel的资料的专业性相比，我对
HP的网卡实在是无力吐嘈。之后研究下再驱动代码吧，有代码有真相。</p>

<p>Have fun.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP状态迁移及状态码]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/08/09/check-tcp-connections/"/>
    <updated>2012-08-09T11:39:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/08/09/check-tcp-connections</id>
    <content type="html"><![CDATA[<p>{% imgpopup /images/post_img/Tcp_state_diagram.svg  50%  %}</p>

<p>想必大家对上图都比较熟悉了，补充下内核里对以上状态码的表示，顺便对源代码做了下改动，每个
状态的代码都补出来了。</p>

<p>{% codeblock tcp_state.h lang:c %}</p>

<p>enum {</p>

<pre><code>TCP_ESTABLISHED = 1,
TCP_SYN_SENT    = 2,
TCP_SYN_RECV    = 3,
TCP_FIN_WAIT1   = 4,
TCP_FIN_WAIT2   = 5,
TCP_TIME_WAIT   = 6
TCP_CLOSE       = 7, 
TCP_CLOSE_WAIT  = 8,
TCP_LAST_ACK    = 9,
TCP_LISTEN      = 10,
TCP_CLOSING     = 11,   /* Now a valid state */

TCP_MAX_STATES  /* Leave at the end! */
</code></pre>

<p>};</p>

<p>{% endcodeblock   %}</p>

<!-- more -->


<p>用<code>cat /proc/net/tcp</code>打印时，输出所有active connections，
<a href="%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E4%BB%8B%E7%BB%8D">http://search.cpan.org/~salva/Linux-Proc-Net-TCP-0.03/lib/Linux/Proc/Net/TCP.pm#The_/proc/net/tcp_documentation</a>。</p>

<p><code>st</code>这一列即代表连接状态，下面该怎么做，你懂的。</p>

<p>要想快速高效的显示连接状态信息，推荐用(ss)[http://stackoverflow.com/questions/11763376/difference-between-netstat-and-ss-in-linux]。
Have fun.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[了解lvs调试]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/07/26/lvs-debug-howto/"/>
    <updated>2012-07-26T21:32:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/07/26/lvs-debug-howto</id>
    <content type="html"><![CDATA[<p>虽然对lvs的实现代码也算是心里有数了，但遇到一些具体问题时还拿不准，这时候就想到了lvs提供的调试功能，
是内核提供的一个选项。一般发行版默认是没打开的，至少我接触的suse enterprice和archlinux都是关闭
了ip_vs_debug，一起来看看怎么启用吧。
{% codeblock kernel config  %}
CONFIG_IP_VS_DEBUG=y
{% endcodeblock %}</p>

<p>在默认的config文件里找到这一项，可能是注释状态，改好后，就可以开始编译内核了，这个选项不是m，必须重新
编译产生新内核，在用<code>mkinitcpio -k kernel_full_name -c config</code>或者<code>mkinitrd -k kernel_full_name
</code>，前面是针对archlinux的，后面是针对suse的，这里有坑，此处略去1000字，反正记得如果用新内核启动，出现
找不到root的情况，八成是initrd做得有问题，fs相关的modules的问题。</p>

<p>新内核一切安好后，就可以玩lvs的调试功能了。先运行下<code>ipvsadm</code>，再<code>lsmod</code>看下<code>ip_vs</code>是否已经加载了。
再后面，就可以检查期待已久的调试选项了
{% codeblock ip_vs debug in /proc %}
/proc/sys/net/ipv4/vs/debug_level
{% endcodeblock %}</p>

<p><code>cat debug_level</code>一下，默认值是0，以后可以通过sysctl来更改默认值，下面讲下这个值的含义:
{% codeblock ip_vs_core.c lang:c %}
 IP_VS_DBG_BUF(1, "Forward ICMP: failed checksum from %s!\n", IP_VS_DBG_ADDR(af, snet));
{% endcodeblock %}
上面是一个debug macro的调用的例子，下面是宏的具体定义：</p>

<p>{% codeblock ip_vs.h lang:c %}</p>

<h1>ifdef CONFIG_IP_VS_DEBUG</h1>

<p>extern int ip_vs_get_debug_level(void);</p>

<h1>define IP_VS_DBG(level, msg...)            \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level())   \
        printk(KERN_DEBUG "IPVS: " msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_RL(msg...)                \</h1>

<pre><code>do {                        \
    if (net_ratelimit())            \
        printk(KERN_DEBUG "IPVS: " msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_PKT(level, pp, skb, ofs, msg)     \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level())   \
    pp-&gt;debug_packet(pp, skb, ofs, msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_RL_PKT(level, pp, skb, ofs, msg)  \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level() &amp;&amp; \
    net_ratelimit())            \
    pp-&gt;debug_packet(pp, skb, ofs, msg);    \
} while (0)
</code></pre>

<h1>else   /<em> NO DEBUGGING at ALL </em>/</h1>

<h1>define IP_VS_DBG(level, msg...)  do {} while (0)</h1>

<h1>define IP_VS_DBG_RL(msg...)  do {} while (0)</h1>

<h1>define IP_VS_DBG_PKT(level, pp, skb, ofs, msg)     do {} while (0)</h1>

<h1>define IP_VS_DBG_RL_PKT(level, pp, skb, ofs, msg)  do {} while (0)</h1>

<h1>endif</h1>

<p>{% endcodeblock %}
可以清晰的看到，打印debug日志的条件是<code>level &lt;= ip_vs_get_debug_level()</code>，这个函数调用的返回值就是
<code>/proc/sys/net/ipv4/vs/debug_level</code>，万物皆文件的理念又体现了。
可见level的值越小，其优先级越高，查了一遍ip_vs的相关代码，level最大值不过12，也就是说，
只要把<code>/proc/sys/net/ipv4/vs/debug_level</code>设置成12，就能保证所有ip_vs的日志输出了，但一般没那个必要。
{% codeblock set debug_level to 12 %}
echo 12 > /proc/sys/net/ipv4/vs/debug_level
{% endcodeblock %}
对了，日志输出是在dmesg。</p>

<p>好了，方法就差不多是这些了，接下来就边看代码，边测试，边看输出吧。
have fun.</p>
]]></content>
  </entry>
  
</feed>
