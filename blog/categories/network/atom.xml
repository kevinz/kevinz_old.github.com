<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: network | Code Mind]]></title>
  <link href="http://gekben.gitcd.com/blog/categories/network/atom.xml" rel="self"/>
  <link href="http://gekben.gitcd.com/"/>
  <updated>2012-08-16T21:56:41+08:00</updated>
  <id>http://gekben.gitcd.com/</id>
  <author>
    <name><![CDATA[Kevin Zeng]]></name>
    <email><![CDATA[kevintech08@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[升级Netxen网卡驱动及firmware]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/08/16/upgrade-the-netxen-nic/"/>
    <updated>2012-08-16T20:15:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/08/16/upgrade-the-netxen-nic</id>
    <content type="html"><![CDATA[<p>在公司做lvs测试用机是HP DL580 G7，除了网卡，其它的配置都不错，网卡是板
载的netxen的1G卡，还有几张netxen的10G卡，观察<code>/proc/interrupts</code>，每块
10G卡对应的中断都只有4个，而且没有识别成tx-0/rx-0的形式，不像支持
multiple queue，不禁使我对SUSE 11
Enterprise SP2的netxen驱动产生的怀疑。<code>dmesg</code>也可以发现有抱怨driver和
firmware的比较旧，于是萌生了升级驱动的想法。</p>

<p>用<code>netxen</code>和<code>nx3031</code>search
了很久而不得，偶然通过<code>nx_nic</code>找到了HP网卡的驱动
<a href="%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80">http://h20000.www2.hp.com/bizsupport/TechSupport/SoftwareIndex.jsp?lang=en&amp;cc=us&amp;prodNameId=3913538&amp;prodTypeId=329290&amp;prodSeriesId=3913537&amp;swLang=8&amp;taskId=135&amp;swEnvOID=4049</a>，包括firmware的自动升级程序和网卡驱动。值得吐一把嘈的是，HP
网站上driver是老旧的，不支持稍新的内核，貌似只支持到2.6.20以前的，编译
时会报<code>net_dev struct</code>相关的错，费了
那么大的力气，搞来个不能用的，之后还是google到了最新的驱动，在一个连域
名都没有的
<a href="ftp">ftp://15.217.49.75/ftp2/pub/softlib2/software1/pubsw-linux/p280489831/</a>
上，haha，真欢乐，HP sucks.按照说明先升级好driver，<code>modprobe nx_nic</code>后，
就可以升级firmware了，要注意的是，把每个网卡都up起来，才能保证所有都升
级成功，我拆掉了一些bond，保证以前是slave状态的卡都up起来之后，经过10
分钟左右，升级成功。重启后<code>dmesg</code>里的complains少了很多，新版本也显示出
来了，还是没识别出multiple queue，编译驱动时也检查了选项，确实没有。同
事也咨询过HP的sales，也不知道多队列网卡为何物，与Intel的资料的专业性相比，我对
HP的网卡实在是无力吐嘈。之后研究下再驱动代码吧，有代码有真相。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TCP状态迁移及状态码]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/08/09/check-tcp-connections/"/>
    <updated>2012-08-09T11:39:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/08/09/check-tcp-connections</id>
    <content type="html"><![CDATA[<p>{% imgpopup /images/post_img/Tcp_state_diagram.svg  50%  %}</p>

<p>想必大家对上图都比较熟悉了，补充下内核里对以上状态码的表示，顺便对源代码做了下改动，每个
状态的代码都补出来了。</p>

<p>{% codeblock tcp_state.h lang:c %}</p>

<p>enum {</p>

<pre><code>TCP_ESTABLISHED = 1,
TCP_SYN_SENT    = 2,
TCP_SYN_RECV    = 3,
TCP_FIN_WAIT1   = 4,
TCP_FIN_WAIT2   = 5,
TCP_TIME_WAIT   = 6
TCP_CLOSE       = 7, 
TCP_CLOSE_WAIT  = 8,
TCP_LAST_ACK    = 9,
TCP_LISTEN      = 10,
TCP_CLOSING     = 11,   /* Now a valid state */

TCP_MAX_STATES  /* Leave at the end! */
</code></pre>

<p>};</p>

<p>{% endcodeblock   %}</p>

<!-- more -->


<p>用<code>cat /proc/net/tcp</code>打印时，输出所有active connections，
<a href="%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E4%BB%8B%E7%BB%8D">http://search.cpan.org/~salva/Linux-Proc-Net-TCP-0.03/lib/Linux/Proc/Net/TCP.pm#The_/proc/net/tcp_documentation</a>。</p>

<p><code>st</code>这一列即代表连接状态，下面该怎么做，你懂的。</p>

<p>Have fun.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[了解lvs调试]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/07/26/lvs-debug-howto/"/>
    <updated>2012-07-26T21:32:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/07/26/lvs-debug-howto</id>
    <content type="html"><![CDATA[<p>虽然对lvs的实现代码也算是心里有数了，但遇到一些具体问题时还拿不准，这时候就想到了lvs提供的调试功能，
是内核提供的一个选项。一般发行版默认是没打开的，至少我接触的suse enterprice和archlinux都是关闭
了ip_vs_debug，一起来看看怎么启用吧。
{% codeblock kernel config  %}
CONFIG_IP_VS_DEBUG=y
{% endcodeblock %}</p>

<p>在默认的config文件里找到这一项，可能是注释状态，改好后，就可以开始编译内核了，这个选项不是m，必须重新
编译产生新内核，在用<code>mkinitcpio -k kernel_full_name -c config</code>或者<code>mkinitrd -k kernel_full_name
</code>，前面是针对archlinux的，后面是针对suse的，这里有坑，此处略去1000字，反正记得如果用新内核启动，出现
找不到root的情况，八成是initrd做得有问题，fs相关的modules的问题。</p>

<p>新内核一切安好后，就可以玩lvs的调试功能了。先运行下<code>ipvsadm</code>，再<code>lsmod</code>看下<code>ip_vs</code>是否已经加载了。
再后面，就可以检查期待已久的调试选项了
{% codeblock ip_vs debug in /proc %}
/proc/sys/net/ipv4/vs/debug_level
{% endcodeblock %}</p>

<p><code>cat debug_level</code>一下，默认值是0，以后可以通过sysctl来更改默认值，下面讲下这个值的含义:
{% codeblock ip_vs_core.c lang:c %}
 IP_VS_DBG_BUF(1, "Forward ICMP: failed checksum from %s!\n", IP_VS_DBG_ADDR(af, snet));
{% endcodeblock %}
上面是一个debug macro的调用的例子，下面是宏的具体定义：</p>

<p>{% codeblock ip_vs.h lang:c %}</p>

<h1>ifdef CONFIG_IP_VS_DEBUG</h1>

<p>extern int ip_vs_get_debug_level(void);</p>

<h1>define IP_VS_DBG(level, msg...)            \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level())   \
        printk(KERN_DEBUG "IPVS: " msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_RL(msg...)                \</h1>

<pre><code>do {                        \
    if (net_ratelimit())            \
        printk(KERN_DEBUG "IPVS: " msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_PKT(level, pp, skb, ofs, msg)     \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level())   \
    pp-&gt;debug_packet(pp, skb, ofs, msg);    \
} while (0)
</code></pre>

<h1>define IP_VS_DBG_RL_PKT(level, pp, skb, ofs, msg)  \</h1>

<pre><code>do {                        \
    if (level &lt;= ip_vs_get_debug_level() &amp;&amp; \
    net_ratelimit())            \
    pp-&gt;debug_packet(pp, skb, ofs, msg);    \
} while (0)
</code></pre>

<h1>else   /<em> NO DEBUGGING at ALL </em>/</h1>

<h1>define IP_VS_DBG(level, msg...)  do {} while (0)</h1>

<h1>define IP_VS_DBG_RL(msg...)  do {} while (0)</h1>

<h1>define IP_VS_DBG_PKT(level, pp, skb, ofs, msg)     do {} while (0)</h1>

<h1>define IP_VS_DBG_RL_PKT(level, pp, skb, ofs, msg)  do {} while (0)</h1>

<h1>endif</h1>

<p>{% endcodeblock %}
可以清晰的看到，打印debug日志的条件是<code>level &lt;= ip_vs_get_debug_level()</code>，这个函数调用的返回值就是
<code>/proc/sys/net/ipv4/vs/debug_level</code>，万物皆文件的理念又体现了。
可见level的值越小，其优先级越高，查了一遍ip_vs的相关代码，level最大值不过12，也就是说，
只要把<code>/proc/sys/net/ipv4/vs/debug_level</code>设置成12，就能保证所有ip_vs的日志输出了，但一般没那个必要。
{% codeblock set debug_level to 12 %}
echo 12 > /proc/sys/net/ipv4/vs/debug_level
{% endcodeblock %}
对了，日志输出是在dmesg。</p>

<p>好了，方法就差不多是这些了，接下来就边看代码，边测试，边看输出吧。
have fun.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[学习万兆以太网，硬件和优化方法]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/07/15/hardware-and-10gb-network/"/>
    <updated>2012-07-15T11:34:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/07/15/hardware-and-10gb-network</id>
    <content type="html"><![CDATA[<p>上周末去杭州参加了<a href="http://adc.taobao.com/">ADC</a>，收获不小，关注的几个
topic都和万兆网有关。最近也在做用LVS替换F5的调研，对性能的要求非常高，
于是开始关注起10Gbe，记录一些资料和心得。</p>

<h2>个人关注点</h2>

<ul>
<li><a href="https://speakerdeck.com/u/gekben/p/by-intel">拥抱万兆以太网时代 - 探索英特尔® 至强® 处理器平台上的应用性能优化</a>

<ul>
<li>图片缩放及gzip的增强</li>
<li>E5-2600 DDIO这个绝对是重点，对网络收发性能提升明显</li>
<li>性价比高的10G BASE-T会逐渐成为主流</li>
</ul>
</li>
<li><a href="https://speakerdeck.com/u/gekben/p/lvs-used-in-taobao">LVS在淘宝环境中的应用</a>

<ul>
<li>FULLNAT模式，对大型网络降低运维难度非常好，但对LVS的改造很大，增加了复杂性</li>
<li>SynProxy，在lvs上实现syn cookie防御机制，不明白为什么要做在这里</li>
<li>taobao也没用上sandy bridge，还是E5640+intel 82599万兆卡</li>
<li>多队列网卡，每个core绑定网卡的一个队列</li>
<li>KeepAlived增强，select->epoll,localaddr</li>
<li>Cluster，因为real server本身无状态，水平扩容好做，OSPF</li>
<li>4/7层结合，lvs+nginx，nginx做7层业务负载</li>
</ul>
</li>
</ul>


<h2>硬件</h2>

<ul>
<li><a href="http://storage.chinabyte.com/447/12275447.shtml">介绍</a>

<ul>
<li><a href="http://en.wikipedia.org/wiki/Sandy_Bridge_%28microarchitecture%29">sandy bridge</a> cpu列表</li>
<li><a href="http://www.intel.com/content/www/us/en/network-adapters/gigabit-network-adapters/ethernet-x520.html">x520</a></li>
<li><a href="http://ark.intel.com/products/58954/Intel-Ethernet-Converged-Network-Adapter-X540-T2">x540</a></li>
<li><a href="http://www.emulex.com/products/10gbe-network-adapters-nic/emulex-branded/oce11102-nt/overview.html">emulex 手头正好有这块卡</a></li>
</ul>
</li>
</ul>


<h2>优化</h2>

<ul>
<li>intel那个slides里有几个建议

<ul>
<li>Process affinity - socket</li>
<li>Interrupts affinity - socket</li>
<li>Disable LLDPAD,IPTABLES,IP6TABLES,SELINUX,IRQBALANCE</li>
</ul>
</li>
<li><a href="http://timetobleed.com/useful-kernel-and-driver-performance-tweaks-for-your-linux-server/">Linux kernel and driver</a></li>
<li><a href="http://www.linuxfoundation.org/collaborate/workgroups/networking">GRO,GSO,TSO...</a></li>
<li>Papers

<ul>
<li><a href="http://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=10Gbe+linux+optimization&amp;source=web&amp;cd=1&amp;ved=0CFQQFjAA&amp;url=http%3A%2F%2Fdocs.redhat.com%2Fdocs%2Fen-US%2FRed_Hat_Enterprise_Linux%2F6%2Fpdf%2FPerformance_Tuning_Guide%2FRed_Hat_Enterprise_Linux-6-Performance_Tuning_Guide-en-US.pdf&amp;ei=3D4RUKmWJ6O3iQed2YHYCw&amp;usg=AFQjCNGRhggpqYuUP0LQwoQE1uofWlN2dQ">RedHat</a></li>
<li><a href="http://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=10Gbe+linux+optimization&amp;source=web&amp;cd=5&amp;ved=0CGUQFjAE&amp;url=http%3A%2F%2Fakashi.ci.i.u-tokyo.ac.jp%2Flab%2Fcmsdesigner%2Fdlfile.php%3Fentryname%3Dpublic%26entryid%3D00096%26fileid%3D00000001%26%2Fysn-camera.pdf&amp;ei=3D4RUKmWJ6O3iQed2YHYCw&amp;usg=AFQjCNGXO4wm2DQ8A6cAg1IsVzjbGjzvDg">Google</a></li>
<li><a href="http://www.kernel.org/doc/ols/2009/ols2009-pages-169-184.pdf">IBM</a></li>
</ul>
</li>
</ul>


<p>先记录到这里，边实践边补充。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LVS peristent代码分析]]></title>
    <link href="http://gekben.gitcd.com/blog/2012/07/11/study-on-lvs-kernel-code/"/>
    <updated>2012-07-11T12:24:00+08:00</updated>
    <id>http://gekben.gitcd.com/blog/2012/07/11/study-on-lvs-kernel-code</id>
    <content type="html"><![CDATA[<p>一直纠结于LVS使用persistent时，是以何为依据，决定一个新的连接请求的命运的：是被persistent管理，去到之前的real server，还是被调度算法重新调度，去到新的real server。
代码里其实写得非常清楚：</p>

<h2>Persistent in LVS(ipvs)</h2>

<ul>
<li>fwmark

<ul>
<li>&lt;IPPROTO_IP,caddr,0,fwmark,0,daddr,0> 这个六元组，最开始是写死的
ip协议，第一个0是cport，第二个0是dport，就是不在乎cport和dport，
这个daddr值得一提，经过debug发现，这个值为<code>0.0.0.fwmark</code>。</li>
</ul>
</li>
<li>Port zero service &lt;protocol,caddr,0,vaddr,0,daddr,0></li>
<li>non Port zero service

<ul>
<li>FTP &lt;caddr,0,vaddr,0,daddr,0></li>
<li>NON-FTP &lt;caddr,0,vaddr,vport,daddr,dport></li>
</ul>
</li>
</ul>


<!-- more -->


<p>  <br/>
如果是transparent mode，这种透明模式一般都是通过fwmark的方式实现，客户
端是不知道vip的存在的，比如用<code>iptables</code>设置了<code>fwmark</code>为3，则访问
http://sina.com.cn时，<code>daddr</code>就是<code>0.0.0.3</code>。</p>

<p>见代码17，21，77，80行。
{% codeblock /net/netfilter/ipvs/ip_vs_core.c lang:c %}</p>

<pre><code>/*
 * As far as we know, FTP is a very complicated network protocol, and
 * it uses control connection and data connections. For active FTP,
 * FTP server initialize data connection to the client, its source port
 * is often 20. For passive FTP, FTP server tells the clients the port
 * that it passively listens to,  and the client issues the data
 * connection. In the tunneling or direct routing mode, the load
 * balancer is on the client-to-server half of connection, the port
 * number is unknown to the load balancer. So, a conn template like
 * &lt;caddr, 0, vaddr, 0, daddr, 0&gt; is created for persistent FTP
 * service, and a template like &lt;caddr, 0, vaddr, vport, daddr, dport&gt;
 * is created for other persistent services.
 */
if (ports[1] == svc-&gt;port) {
    /* Check if a template already exists */
    if (svc-&gt;port != FTPPORT)
        ct = ip_vs_ct_in_get(svc-&gt;af, iph.protocol, &amp;snet, 0,
                     &amp;iph.daddr, ports[1]); /* &lt;caddr,0,vaddr,vport,daddr,dport&gt; */
    else
        ct = ip_vs_ct_in_get(svc-&gt;af, iph.protocol, &amp;snet, 0,
                     &amp;iph.daddr, 0);        /* &lt;caddr,0,vaddr,0,daddr,0&gt; */

    if (!ct || !ip_vs_check_template(ct)) {
        /*
         * No template found or the dest of the connection
         * template is not available.
         */
        dest = svc-&gt;scheduler-&gt;schedule(svc, skb);
        if (dest == NULL) {
            IP_VS_DBG(1, "p-schedule: no dest found.\n");
            return NULL;
        }

        /*
         * Create a template like &lt;protocol,caddr,0,
         * vaddr,vport,daddr,dport&gt; for non-ftp service,
         * and &lt;protocol,caddr,0,vaddr,0,daddr,0&gt;
         * for ftp service.
         */
        if (svc-&gt;port != FTPPORT)
            ct = ip_vs_conn_new(svc-&gt;af, iph.protocol,
                        &amp;snet, 0,
                        &amp;iph.daddr,
                        ports[1],
                        &amp;dest-&gt;addr, dest-&gt;port,
                        IP_VS_CONN_F_TEMPLATE,
                        dest);
        else
            ct = ip_vs_conn_new(svc-&gt;af, iph.protocol,
                        &amp;snet, 0,
                        &amp;iph.daddr, 0,
                        &amp;dest-&gt;addr, 0,
                        IP_VS_CONN_F_TEMPLATE,
                        dest);
        if (ct == NULL)
            return NULL;

        ct-&gt;timeout = svc-&gt;timeout;
    } else {
        /* set destination with the found template */
        dest = ct-&gt;dest;
    }
    dport = dest-&gt;port;
} else {
    /*
     * Note: persistent fwmark-based services and persistent
     * port zero service are handled here.
     * fwmark template: &lt;IPPROTO_IP,caddr,0,fwmark,0,daddr,0&gt;
     * port zero template: &lt;protocol,caddr,0,vaddr,0,daddr,0&gt;
     */
    if (svc-&gt;fwmark) {
        union nf_inet_addr fwmark = {
            .ip = htonl(svc-&gt;fwmark)
        };

        ct = ip_vs_ct_in_get(svc-&gt;af, IPPROTO_IP, &amp;snet, 0,
                     &amp;fwmark, 0); /* &lt;IPPROTO_IP,caddr,0,fwmark,0,daddr,0&gt; */
    } else
        ct = ip_vs_ct_in_get(svc-&gt;af, iph.protocol, &amp;snet, 0,
                     &amp;iph.daddr, 0); /* &lt;protocol,caddr,0,vaddr,0,daddr,0&gt; */

    if (!ct || !ip_vs_check_template(ct)) {
        /*
         * If it is not persistent port zero, return NULL,
         * otherwise create a connection template.
         */
        if (svc-&gt;port)
            return NULL;

        dest = svc-&gt;scheduler-&gt;schedule(svc, skb);
        if (dest == NULL) {
            IP_VS_DBG(1, "p-schedule: no dest found.\n");
            return NULL;
        }

        /*
         * Create a template according to the service
         */
        if (svc-&gt;fwmark) {
            union nf_inet_addr fwmark = {
                .ip = htonl(svc-&gt;fwmark)
            };

            ct = ip_vs_conn_new(svc-&gt;af, IPPROTO_IP,
                        &amp;snet, 0,
                        &amp;fwmark, 0,
                        &amp;dest-&gt;addr, 0,
                        IP_VS_CONN_F_TEMPLATE,
                        dest);
        } else
            ct = ip_vs_conn_new(svc-&gt;af, iph.protocol,
                        &amp;snet, 0,
                        &amp;iph.daddr, 0,
                        &amp;dest-&gt;addr, 0,
                        IP_VS_CONN_F_TEMPLATE,
                        dest);
        if (ct == NULL)
            return NULL;

        ct-&gt;timeout = svc-&gt;timeout;
    } else {
        /* set destination with the found template */
        dest = ct-&gt;dest;
    }
    dport = ports[1];
}
</code></pre>

<p>{% endcodeblock %}</p>
]]></content>
  </entry>
  
</feed>
